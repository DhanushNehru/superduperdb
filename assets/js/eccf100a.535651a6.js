"use strict";(self.webpackChunknewdocs=self.webpackChunknewdocs||[]).push([[744],{3905:(e,n,t)=>{t.d(n,{Zo:()=>l,kt:()=>f});var r=t(67294);function o(e,n,t){return n in e?Object.defineProperty(e,n,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[n]=t,e}function a(e,n){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);n&&(r=r.filter((function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable}))),t.push.apply(t,r)}return t}function i(e){for(var n=1;n<arguments.length;n++){var t=null!=arguments[n]?arguments[n]:{};n%2?a(Object(t),!0).forEach((function(n){o(e,n,t[n])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):a(Object(t)).forEach((function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(t,n))}))}return e}function s(e,n){if(null==e)return{};var t,r,o=function(e,n){if(null==e)return{};var t,r,o={},a=Object.keys(e);for(r=0;r<a.length;r++)t=a[r],n.indexOf(t)>=0||(o[t]=e[t]);return o}(e,n);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);for(r=0;r<a.length;r++)t=a[r],n.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(o[t]=e[t])}return o}var p=r.createContext({}),d=function(e){var n=r.useContext(p),t=n;return e&&(t="function"==typeof e?e(n):i(i({},n),e)),t},l=function(e){var n=d(e.components);return r.createElement(p.Provider,{value:n},e.children)},m="mdxType",c={inlineCode:"code",wrapper:function(e){var n=e.children;return r.createElement(r.Fragment,{},n)}},u=r.forwardRef((function(e,n){var t=e.components,o=e.mdxType,a=e.originalType,p=e.parentName,l=s(e,["components","mdxType","originalType","parentName"]),m=d(t),u=o,f=m["".concat(p,".").concat(u)]||m[u]||c[u]||a;return t?r.createElement(f,i(i({ref:n},l),{},{components:t})):r.createElement(f,i({ref:n},l))}));function f(e,n){var t=arguments,o=n&&n.mdxType;if("string"==typeof e||o){var a=t.length,i=new Array(a);i[0]=u;var s={};for(var p in n)hasOwnProperty.call(n,p)&&(s[p]=n[p]);s.originalType=e,s[m]="string"==typeof e?e:o,i[1]=s;for(var d=2;d<a;d++)i[d]=t[d];return r.createElement.apply(null,i)}return r.createElement.apply(null,t)}u.displayName="MDXCreateElement"},44362:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>p,contentTitle:()=>i,default:()=>c,frontMatter:()=>a,metadata:()=>s,toc:()=>d});var r=t(87462),o=(t(67294),t(3905));const a={},i=void 0,s={unversionedId:"use_cases/items/video_search",id:"use_cases/items/video_search",title:"video_search",description:"Create VectorIndex with an indexing and compatible listener",source:"@site/content/use_cases/items/video_search.md",sourceDirName:"use_cases/items",slug:"/use_cases/items/video_search",permalink:"/docs/use_cases/items/video_search",draft:!1,editUrl:"https://github.com/SuperDuperDB/superduperdb/tree/main/docs/content/use_cases/items/video_search.md",tags:[],version:"current",frontMatter:{},sidebar:"useCasesSidebar",previous:{title:"Transfer learning using Sentence Transformers and Scikit-Learn",permalink:"/docs/use_cases/items/transfer_learning"},next:{title:"Cataloguing voice-memos for a self managed personal assistant",permalink:"/docs/use_cases/items/voice_memos"}},p={},d=[{value:"Create VectorIndex with an indexing and compatible listener",id:"create-vectorindex-with-an-indexing-and-compatible-listener",level:2}],l={toc:d},m="wrapper";function c(e){let{components:n,...t}=e;return(0,o.kt)(m,(0,r.Z)({},l,t,{components:n,mdxType:"MDXLayout"}),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"# installs\n!pip install superduperdb\n!pip install opencv-python\n!pip install git+https://github.com/openai/CLIP.git\n")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"import cv2\nimport requests\nimport os\nimport numpy as np\nfrom tqdm import tqdm\n\nimport pymongo\n\nimport superduperdb\nfrom superduperdb import superduper\nimport glob\nfrom PIL import Image\nfrom superduperdb.ext.pillow.image import pil_image as i\n\nfrom superduperdb.container.document import Document as D\nfrom superduperdb.container.model import Model\nfrom superduperdb.container.schema import Schema\nfrom superduperdb.db.mongodb.query import Collection\nfrom superduperdb.ext.torch.tensor import tensor\nfrom superduperdb.ext.torch.model import TorchModel\nimport torch\n")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"s = Schema(identifier='myschema', fields={'image':i})\n")),(0,o.kt)("h1",{id:"create-a-superduperdb-instance"},"Create a superduper",(0,o.kt)("inlineCode",{parentName:"h1"},"db")," instance"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'import os\n\n# Uncomment one of the following lines to use a bespoke MongoDB deployment\n# For testing the default connection is to mongomock\n\nmongodb_uri = os.getenv("MONGODB_URI","mongomock://test")\n# mongodb_uri = "mongodb://localhost:27017"\n# mongodb_uri = "mongodb://superduper:superduper@mongodb:27017/documents"\n# mongodb_uri = "mongodb://<user>:<pass>@<mongo_cluster>/<database>"\n# mongodb_uri = "mongodb+srv://<username>:<password>@<atlas_cluster>/<database>"\n\n# Super-Duper your Database!\nfrom superduperdb import superduper\ndb = superduper(mongodb_uri)\n')),(0,o.kt)("h1",{id:"sample-video-url-json"},"Sample video url json"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"url = 'https://github.com/SuperDuperDB/superduperdb/assets/138251983/99f35f54-d4b0-40e6-a22d-41043d7bd384'\n")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"db.execute(Collection('video_frames').insert_many([D({'uri':url})]))\n")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"class Video2Images:\n    def download_video(self, url, output_file):\n        response = requests.get(url)\n        if response.status_code == 200:\n            with open(output_file, 'wb') as f:\n                f.write(response.content)\n        else:\n            print(f\"Failed to download video from {url}\")\n            return False\n        return True\n\n    def save_frames_from_video(self, video_file, output_folder, threshold=10, min_frame_interval=300):\n        cap = cv2.VideoCapture(video_file)\n        if not cap.isOpened():\n            print(\"Error: Could not open video file.\")\n            return\n    \n        prev_frame = None\n        frame_count = 0\n    \n        os.makedirs(output_folder, exist_ok=True)\n        fps = cap.get(cv2.CAP_PROP_FPS)\n    \n        while True:\n            ret, frame = cap.read()\n            if not ret:\n                break\n            gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n            current_timestamp = frame_count // fps\n            all_zeros = not np.any(np.asarray(gray_frame))\n            frame_count += 1\n            if all_zeros:\n                continue\n            \n            if prev_frame is not None:\n                \n                frame_diff = cv2.absdiff(gray_frame, prev_frame)\n                mean_diff = np.mean(frame_diff)\n                if mean_diff > threshold and frame_count > min_frame_interval:\n                    frame_filename = f\"{output_folder}/frame_{current_timestamp}.jpg\"\n                    cv2.imwrite(frame_filename, frame)\n    \n            \n            prev_frame = gray_frame\n    \n        cap.release()\n        cv2.destroyAllWindows()\n\n    def __call__(self, url, name='landscape.mp4'):\n        path = '.videos'\n        video_path = os.path.join(path, name)\n        self.download_video(url, video_path)\n        frame_path = os.path.join(path, 'frames')\n        self.save_frames_from_video(video_path, frame_path)\n        docs = []\n        for image in  glob.glob(f'{frame_path}/*.jpg')[:2]:\n            current_timestamp = os.path.split(image)[-1].split('.')[0].split('_')[-1]\n            image_array = cv2.imread(image)\n            \n            doc = {'image': Image.fromarray(image_array[:,:,::-1]), 'current_timestamp': current_timestamp, 'frame_uri':image}\n            docs.append(doc)\n        return docs\n")),(0,o.kt)("h1",{id:"create-a-listener-which-will-continously-download-video-urls-and-save-best-frames-into-other-collection"},"Create a Listener which will continously download video urls and save best frames into other collection."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"from superduperdb.container.listener import Listener\n\n!mkdir .videos\n\nvideo2images = Model(identifier='video2images', object=Video2Images(), flatten=True, model_update_kwargs={'document_embedded':False}, output_schema=s)\ndb.add(\n   Listener(\n       model=video2images,\n       select=Collection(name='video_frames').find(),\n       key='uri',\n   )\n)\n")),(0,o.kt)("h1",{id:"create-clip-model"},"Create CLIP model"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"import clip\n")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"model, preprocess = clip.load(\"RN50\", device='cpu')\nt = tensor(torch.float, shape=(512,))\n")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"visual_model = TorchModel(\n    identifier='clip_image',\n    preprocess=preprocess,\n    object=model.visual,\n    encoder=t,\n)\ntext_model = TorchModel(\n    identifier='clip_text',\n    object=model,\n    preprocess=lambda x: clip.tokenize(x)[0],\n    forward_method='encode_text',\n    encoder=t,\n    device='cpu',\n    preferred_devices=None\n)\n")),(0,o.kt)("h2",{id:"create-vectorindex-with-an-indexing-and-compatible-listener"},"Create VectorIndex with an indexing and compatible listener"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"from superduperdb.container.vector_index import VectorIndex\nfrom superduperdb.container.listener import Listener\nfrom superduperdb.ext.openai.model import OpenAIEmbedding\nfrom superduperdb.db.mongodb.query import Collection\n\ndb.add(\n    VectorIndex(\n        identifier='VideoSearchIndex',\n        indexing_listener=Listener(\n            model=visual_model,\n            key='_outputs.uri.video2images.image',\n            select=Collection(name='_outputs.uri.video2images').find(),\n        ),\n        compatible_listener=Listener(\n            model=text_model,\n            key='text',\n            select=None,\n            active=False\n        )\n    )\n)\n")),(0,o.kt)("h1",{id:"test-vector-search-by-quering-a-text-against-saved-frames"},"Test vector search by quering a text against saved frames."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"#  We will get the timestamp of the resultant frame and start the video from this timestamp.\nout = db.execute(\n    Collection('_outputs.uri.video2images').like(D({'text': 'A desert like place.'}), vector_index='VideoSearchIndex', n=1).find()\n)\nresult = [c for c in out]\nresult = result[0].outputs('uri', 'video2images')\nsearch_timestamp = result['current_timestamp']\n")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"display(result['image'])\n")),(0,o.kt)("h1",{id:"start-the-video-from-the-resultant-timestamp"},"Start the video from the resultant timestamp"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'from IPython.display import display, HTML\nvideo_html = f"""\n<video width="640" height="480" controls>\n  <source src="./videos/landscape.mp4" type="video/mp4">\n</video>\n<script>\nvar video = document.querySelector(\'video\');\nvideo.currentTime = {search_timestamp};\nvideo.play();\n<\/script>\n"""\n\ndisplay(HTML(video_html))\n')))}c.isMDXComponent=!0}}]);